---
title: "Assignment 4 - Applying meta-analytic priors"
author: "Riccardo Fusaroli"
date: "3/16/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("~/Desktop/R code semester 4/Computational-Modelling-A4")
library(readxl)
library(brms)
library(brmstools)
library(data.table)
library(rethinking)
library(rstan)
library(bayesplot)
library(cowplot)
metadata = read_excel("Assignment4MetaData.xlsx")
pitch_data = read_excel("Assignment4PitchDatav2.xlsx")


```

## Assignment 4

In this assignment we do the following:
- we reproduce the meta-analysis of pitch SD from last semester in a Bayesian framework
- we reproduce the pitch SD in schizophrenia analysis from last semester using both a conservative and a meta-analytic prior
- we assess the difference in model quality and estimates using the two priors.

The questions you need to answer are: What are the consequences of using a meta-analytic prior? Evaluate the models with conservative and meta-analytic priors. Discuss the effects on estimates. Discuss the effects on model quality. Discuss the role that meta-analytic priors should have in scientific practice. Should we systematically use them? Do they have drawbacks? Should we use them to complement more conservative approaches? How does the use of meta-analytic priors you suggest reflect the skeptical and cumulative nature of science?


### Step by step suggestions

Step 1: Reproduce the meta-analysis of pitch sd from previous studies of voice in schizophrenia
- the data is available as Assignment4MetaData.xlsx
- Effect size (cohen's d), sd and variance are already calculated (you're welcome!)
- Since we're only interested in getting a meta-analytic effect size, let's take a shortcut and use bromance magic (brms): https://mvuorre.github.io/post/2016/2016-09-29-bayesian-meta-analysis/

```{r}
M=brm(MeanES|se(SdES)~1+(1|StudyRef),data=metadata,cores=2,chain=2,iter = 2000)

summary(M)

#We want the mean effect size of the studies and keep information on how sure they are (standard error of their sd effect size)
#We think the studies are evaluating the same phenomenon - but there is some random variation - therefore we put the random effect (1|StudyRef)
#there is uncertainty in MEanES and it can be found as the SE of sdES
#cores - how many cores in the computer to split the processing on, safe is 2
#chain - safe to put 2
#ITER - safe to put 2000
#Priors - we are trying to estimate a prior but we still need a prior for that - brms calculates the best prior so we don't have to put it in ourselves (but they can be changed)

forest(M,show_data = TRUE,av_name = "Effect size")


```

Step 2: Prepare the pitch SD data from last year
- the data is available as Assignment4PitchData.csv (thanks Celine)
- We do not know how to build random effects, yet. So we need to simplify the dataset to avoid interdependence between datapoint: How?
- Also, let's standardize the data, so that they are compatible with our meta-analytic prior (Cohen's d is measured in SDs).

```{r}
#get an overall sd from the pitch on all the trials 
overall_PitchSd = aggregate(pitch_data[, 7], list(pitch_data$ID_unique), mean)
#change the column names 
setnames(overall_PitchSd, "Group.1", "ID_unique")
setnames(overall_PitchSd, "PitchSD", "Overall_PitchSD")

#merge with full data set
Pitch_data_new = merge(overall_PitchSd, pitch_data, by = "ID_unique")

#use only necessary columns
final_pitch_data = data.frame(Pitch_data_new$ID_unique, Pitch_data_new$Overall_PitchSD, Pitch_data_new$diagnosis)

#remove duplicates
final_pitch_data = final_pitch_data[!duplicated(final_pitch_data), ]

#setnames 
setnames(final_pitch_data, "Pitch_data_new.ID_unique", "ID_unique")
setnames(final_pitch_data, "Pitch_data_new.Overall_PitchSD", "Overall_PitchSD")
setnames(final_pitch_data, "Pitch_data_new.diagnosis", "diagnosis")

#standardize pitch sd
final_pitch_data$Overall_PitchSD.s = (final_pitch_data$Overall_PitchSD - mean(final_pitch_data$Overall_PitchSD))/sd(final_pitch_data$Overall_PitchSD)

```


Step 3: Build a regression model predicting Pitch SD from Diagnosis.
- how is the outcome distributed? (likelihood function)

Looking at the data it seems that the outcome variable (standardized pitch SD) is long-tailed as many interaction dominant biological phenomena. Nonetheless we don't know much about the underlying process and choose a normal distribution as it tend to be the best guess.

- how are the parameters of the likelihood distribution distributed?
Alpha and beta are normally distributed and sigma is a Cauchy distribution. A (half)cauchy distribution starts high at 0 and goes down with a heavy tail letting the error be high but decreasingly probable

Which predictors should they be conditioned on?
Mu should be conditioned on diagnosis, we could consider also letting sigma be conditioned on diagnosis as we expect people with schizophrenia to have more variance in their pitchSDs. 

- use a skeptical/conservative prior for the effects of diagnosis. Remember you'll need to motivate it.

We don't know much about alpha and beta - therefore we assume that they are normally distributed. We have standadized the outcome and therefore we choose mean 0. SD should be conservative therefore we choose 1. 

#1 is not conservative, it covers all values (spanding from -3 to 3), a more conservative prior is 0.1, we expect difference between the two groups to be 0 +- 0.3 SDs (it follows from the Gaussian that most probable values are in between -3 SD to 3SD) 

- Describe and plot the estimates. Evaluate model quality

```{r}
#Model of pitch

model_pitchSD <- map(
    alist(
        Overall_PitchSD.s ~ dnorm( mu , sigma ) ,
        mu <- a + b*diagnosis,
        a ~ dnorm( 0 , 1 ) ,
        b ~ dnorm( 0 , 0.1 ) ,
        sigma ~ dcauchy( 0 , 2 )
    ) ,
    data=final_pitch_data )
precis(model_pitchSD)
plot(precis(model_pitchSD))

#simulate data from model
sim.pitch = sim(model_pitchSD, data = final_pitch_data, n = 1000)

#using bayesplot
pp_check_model2 = pp_check(final_pitch_data$Overall_PitchSD.s, sim.pitch, ppc_dens_overlay)

#poterior predictive plot on top of raw data
dens(sim.pitch, col = "red", xlim = c(-5, 5), ylim = c(0,1),  xlab = "PitchSD")
par(new=TRUE)
dens(final_pitch_data$Overall_PitchSD.s, xlim = c(-5, 5), ylim = c(0,1), xlab = "PitchSD")
title("PitchSD predicted ~ diagnosis")


#We could also make sigma depend on diagnosis - schizophrenia might have more variance than controls, log(sigma)=alphasigma + betasigma*diagnosis, log because then we get positive numbers, we would need priors for alphasigma and betasigma as well

#Random effects, mu = a[participant] + b[participants]*diagnosis, lets a and b vary according to participants, but still what we know about the other participants can tell us something about the one
# Instead of a[p]~normal(0,1) we can put a[p]~Ã±ormal(a,1) a~normal(0,1) so that there is an underlying alpha common to all - or we can even make one alpha for schizophrenia and one for controls




```


Step 4: Now re-run the model with the meta-analytic prior
- Describe and plot the estimates. Evaluate model quality

```{r}
#Model with meta priors

model_pitchSD_meta <- map(
    alist(
        Overall_PitchSD.s ~ dnorm( mu , sigma ) ,
        mu <- a + b*diagnosis,
        a ~ dnorm( 0 , 1 ) ,
        b ~ dnorm( -0.6 , 0.27 ) ,
        sigma ~ dcauchy( 0 , 2 )
    ) ,
    data=final_pitch_data )
precis(model_pitchSD_meta)
plot(precis(model_pitchSD_meta))

#simulate
sim.pitch.model3 = sim(model_pitchSD_meta, data = final_pitch_data, n = 1000)

pp_check_model3 = pp_check(final_pitch_data$Overall_PitchSD.s, sim.pitch.model3, ppc_dens_overlay)

#posterior predictive plot
dens(sim.pitch.model3, col = "red", xlim = c(-5, 5), ylim = c(0,1),  xlab = "PitchSD")
par(new=TRUE)
dens(final_pitch_data$Overall_PitchSD.s, xlim = c(-5, 5), ylim = c(0,1), xlab = "PitchSD")
title("PitchSD predicted ~ diagnosis")

```

Step 5: Compare the models
- Plot priors and posteriors of the diagnosis effect in both models
- Compare posteriors between the two models
- Compare their relative distance from truth (WAIC)
- Discuss how they compare and whether any of them is best.

```{r}
#compare plots of predictive posteriors and data 
plot_grid(pp_check_model2, pp_check_model3, labels = c('Model2_Conservative', 'Model3_Metaanalytic'))



x <- seq(-3,3, length=1e5)

y.s <- dnorm(x, 0, 0.1) #sceptical
y.m <- dnorm(x, -0.6, 0.32) #meta
prior_df <- data.frame(x = rep(x,2), y = c(y.s, y.m), prior = c(rep("sceptical", length(y.s)),
                                                                   rep("meta", length(y.m))
                                                                   ))
ggplot(prior_df, aes(x = x, y = y, color = prior)) + geom_line() 

#As we expected: sceptical is more narrow. Meta has a lower mean than original


#plot posterior
post_samples <- c(posterior_samples(model_pitchSD)$b_diagnosis, posterior_samples(model_pitchSD_meta)$b_diagnosis) #how do I extract more than a 1000 per
post_df <- data.frame(post_samples = post_samples, model = c(rep("sceptical", 1000),
                                                                   rep("meta", 1000)
                                                                   ))
ggplot(post_df, aes(x = post_samples, color = model)) + geom_density(adjust = 1)



#compare the posteriors
plot(coeftab(model_pitchSD,model_pitchSD_meta))


WAIC(model_pitchSD)
WAIC(model_pitchSD_meta)

compare(model_pitchSD,model_pitchSD_meta)



#compaing pp checks = looks very similar. looking at the estimates the estimates look different though but the effect might be compensated by moving the intercept 
#doesn't look like one model is better than the other 
#WAIC test put a very tiny advantage to model with meta analytic priors but the se being sd being sp big that this is not difference that matter. If we have to choose between the two models, we could use a conceptual argument to use the model with meta analytic priors. We have a lot of extra knowledge from previous studies and it makes sence to include this knowledge.  



```


Step 6: Prepare a nice write up of the analysis and answer the questions at the top.

Optional step 7: how skeptical should a prior be?
- Try different levels of skepticism and compare them using WAIC.

Optional step 8: Include other predictors
- Do age, gender and education improve the model?
- Should they be main effects or interactions?

Optional step 9: Bromance magic.
- explore the bromance code below including random effects (by default with weakly informative priors)
- learn how to change the prior
- explore effects of trial, age, gender, including the appropriate random slopes
- compare the models you created using WAIC and posterior predictive check (pp_check())


```{r}

brm_out <- brm(PitchSD ~ 1 + Diagnosis  +(1|ID_unique/Study), # Outcome as a function of the predictors as in lme4. 
               data=Data, # Define the data
               family=gaussian(), # Define the family. 
               iter = 5000, warmup = 2000, cores = 4)
summary(brm_out1)
plot(brm_out1)


curve( dnorm( x , -0.6 , 0.33 ) , from=-3 , to=3 ) 
par(new=TRUE) 
dens(model_pitchSD)
```

